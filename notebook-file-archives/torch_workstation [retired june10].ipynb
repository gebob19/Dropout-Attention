{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from language_structure import *\n",
    "from train import batch_iter, load\n",
    "from model import *\n",
    "\n",
    "base = Path('../aclImdb')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "traindf = pd.read_csv('train.csv')\n",
    "lang = load_model()\n",
    "lang = lang.top_n_words_model(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import to_input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Example from dataset\n",
    "batch_size = 3\n",
    "df = traindf[traindf.file_length < 25]\n",
    "for sents, targets in batch_iter(lang, df, batch_size, shuffle=True):\n",
    "    break\n",
    "max(map(len, sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this movie will always be a broadway and movie classic as long as there are still people who sing dance and act .'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([lang.id2word[d] for d in [lang.get_id(w) for w in sents[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed, embed_size = lang.n_words, 10\n",
    "embedding = nn.Embedding(n_embed, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3, 10])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, lengths = to_input_tensor(lang, sents, device)\n",
    "em = embedding(x)\n",
    "em.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as usual sean connery does a great job . lawrence fishburn is good but i have a hard time not seeing him as ike turner .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 3\n",
    "gru = nn.GRU(embed_size, hidden_size, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3, 3])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nn.utils.rnn.pack_padded_sequence(em, lengths)\n",
    "output, hidden = gru(x)\n",
    "output, _ = nn.utils.rnn.pad_packed_sequence(output)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.transpose(0, 1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_attn_mask = torch.ones_like(sent_encoded)\n",
    "old_attn_mask[0, :] = -float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 26, 26]), torch.Size([3, 26, 3]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape, sent_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_vec = attn.unsqueeze(-1) * sent_encoded\n",
    "attn_vec = torch.sum(attn_vec, 1)\n",
    "attn_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([23, 6]),\n",
       " tensor([-0.0985, -0.1809,  0.0910, -0.5316, -0.2783,  0.4990],\n",
       "        grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hidden = torch.cat([attn_vec, sent_encoded], dim=1)\n",
    "total_hidden.shape, total_hidden[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0106,  0.0033,  0.0150],\n",
       "         [ 0.0165,  0.0026,  0.0247],\n",
       "         [ 0.0221,  0.0076,  0.0154],\n",
       "         [ 0.0134,  0.0133,  0.0043],\n",
       "         [ 0.0035, -0.0250, -0.0034],\n",
       "         [ 0.0163, -0.0103, -0.0057],\n",
       "         [ 0.0052, -0.0023,  0.0185],\n",
       "         [ 0.0243, -0.0045,  0.0175],\n",
       "         [ 0.0117, -0.0337,  0.0093],\n",
       "         [ 0.0032,  0.0010,  0.0072],\n",
       "         [ 0.0133,  0.0166, -0.0025],\n",
       "         [ 0.0139,  0.0137, -0.0042],\n",
       "         [-0.0316,  0.0124, -0.0029],\n",
       "         [-0.0768, -0.0651,  0.0015],\n",
       "         [-0.0375, -0.0340,  0.0034],\n",
       "         [-0.0180, -0.0215, -0.0003],\n",
       "         [-0.0614, -0.0607, -0.0004],\n",
       "         [ 0.0076, -0.0115, -0.0035],\n",
       "         [ 0.0223, -0.0049, -0.0006],\n",
       "         [-0.0208,  0.0176,  0.0010],\n",
       "         [-0.0242,  0.0325,  0.0051],\n",
       "         [-0.0120, -0.0280, -0.0086]], grad_fn=<MulBackward0>),\n",
       " torch.Size([23, 3]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.unsqueeze(-1) * sent_encoded, sent_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_vec = torch.sum(attn.unsqueeze(-1) * sent_encoded, dim=0)\n",
    "attn_vec.shape # hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0985, -0.1809,  0.0910, -0.5316, -0.2783,  0.4990],\n",
       "        grad_fn=<CatBackward>), torch.Size([6]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hidden = torch.cat([attn_vec, word_encoded.squeeze()])\n",
    "total_hidden, total_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 1\n",
    "lin = nn.Linear(2 * hidden_size, n_classes)\n",
    "# torch.sigmoid(lin(total_hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Self-Attention RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_encoded = output.transpose(0, 1)[0]\n",
    "\n",
    "# Make Attention Mask\n",
    "attn_mask = torch.eye(max(lengths)) * -float('Inf')\n",
    "attn_mask[torch.isnan(attn_mask)] = 0\n",
    "attn_mask[attn_mask == 0] = 1\n",
    "\n",
    "# Attention MM + Softmax\n",
    "attn = torch.mm(sent_encoded, sent_encoded.transpose(0, 1))\n",
    "attn = torch.softmax(attn * attn_mask, dim=1)\n",
    "\n",
    "# Sum along \n",
    "attn_vec = attn.unsqueeze(-1) * sent_encoded\n",
    "attn_vec = torch.sum(attn_vec, 1)\n",
    "\n",
    "total_hidden_single = torch.cat([attn_vec, sent_encoded], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Self-Attention RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 24, 13]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, lengths = to_input_tensor(lang, sents, device)\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 1., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = torch.eye(max(lengths))\n",
    "attn_mask = torch.stack([I] * batch_size)\n",
    "for i, l in zip(list(range(batch_size)), lengths):\n",
    "    attn_mask[i, :, l:] = 1\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, dtype=torch.uint8)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_encoded = output.transpose(0, 1)\n",
    "\n",
    "attn_mask = torch.eye(max(lengths))\n",
    "\n",
    "attn = torch.bmm(sent_encoded, sent_encoded.transpose(1, 2))\n",
    "attn.data.masked_fill_(attn_mask.byte(), -float('inf'))\n",
    "\n",
    "attn = torch.softmax(attn, dim=2)\n",
    "attn[torch.isnan(attn)] = 0\n",
    "\n",
    "attn_vec = attn.unsqueeze(-1) * sent_encoded.unsqueeze(1)\n",
    "attn_vec = attn_vec.sum(-2)\n",
    "\n",
    "total_hidden = torch.cat([attn_vec, sent_encoded], dim=-1)\n",
    "(total_hidden[0] == total_hidden_single).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 26])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hidden = total_hidden.transpose(-1, -2)\n",
    "total_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_hidden, idx = torch.max(total_hidden, -1)\n",
    "max_hidden.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAttention(nn.Module):\n",
    "    def __init__(self, language, device, embed_dim, hidden_dim, num_embed, n_classes):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.language = language \n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embed, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, bias=True)\n",
    "        self.classify = nn.Linear(2 * hidden_dim, n_classes)\n",
    "        \n",
    "    def forward(self, sents):\n",
    "        # Embed the sequence\n",
    "        x, lengths = to_input_tensor(self.language, sents, self.device)\n",
    "        x_embed = self.embedding(x)\n",
    "        # RNN encoding\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x_embed, lengths)\n",
    "        x, _ = self.gru(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x)\n",
    "\n",
    "        # batch, seq, hidden\n",
    "        x = x.transpose(0, 1)\n",
    "        # attention mask \n",
    "        attn_mask = torch.eye(max(lengths)) * -float('Inf')\n",
    "        attn_mask[torch.isnan(attn_mask)] = 0\n",
    "        attn_mask[attn_mask == 0] = 1\n",
    "\n",
    "        # apply attention over RNN outputs\n",
    "        attn = torch.bmm(x, x.transpose(1, 2))\n",
    "        attn = torch.softmax(attn * attn_mask, dim=2)\n",
    "        attn[torch.isnan(attn)] = 0\n",
    "        attn_vec = attn.unsqueeze(-1) * x.unsqueeze(1)\n",
    "        attn_vec = attn_vec.sum(-2)\n",
    "        attn_out = torch.cat([attn_vec, x], dim=-1)\n",
    "        \n",
    "        # max pool over sequence \n",
    "        attn_out = attn_out.transpose(-1, -2)\n",
    "        max_vec, _ = torch.max(attn_out, -1)\n",
    "        max_vec = max_vec.unsqueeze(-2)\n",
    "        \n",
    "        # binary classification activ.\n",
    "        y = torch.sigmoid(self.classify(max_vec)).squeeze()\n",
    "        return y        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Self_Attention(SaveModel):\n",
    "    def __init__(self, language, device, batch_size, embed_dim, hidden_dim, num_embed, n_classes):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.language = language \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embed, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, bias=True)\n",
    "        self.classify = nn.Linear(2 * hidden_dim, n_classes)\n",
    "        self.attention = RNNAttention()\n",
    "        \n",
    "    def forward(self, sents):\n",
    "        # Embed the sequence\n",
    "        x, lengths = to_input_tensor(self.language, sents, self.device)\n",
    "        x_embed = self.embedding(x)\n",
    "        # RNN encoding\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x_embed, lengths)\n",
    "        x, _ = self.gru(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x)\n",
    "        \n",
    "        x = x.transpose(0, 1)\n",
    "        # get attention over RNN outputs \n",
    "        I = torch.eye(max(lengths))\n",
    "        attn_mask = torch.stack([I] * self.batch_size)\n",
    "        for i, l in zip(list(range(self.batch_size)), lengths):\n",
    "            attn_mask[i, :, l:] = 1\n",
    "            attn_mask[i, l:, :] = 1\n",
    "        \n",
    "        attn = self.attention(x, attn_mask)\n",
    "        attn_vec = attn.unsqueeze(-1) * x.unsqueeze(1)\n",
    "        attn_vec = attn_vec.sum(-2)\n",
    "        attn_out = torch.cat([attn_vec, x], dim=-1)\n",
    "        \n",
    "        # max pool over sequence \n",
    "        attn_out = attn_out.transpose(-1, -2)\n",
    "        max_vec, _ = torch.max(attn_out, -1)\n",
    "        max_vec = max_vec.unsqueeze(-2)\n",
    "        \n",
    "        # binary classification activ.\n",
    "        y = torch.sigmoid(self.classify(max_vec)).squeeze()\n",
    "        return y        \n",
    "    \n",
    "class RNNAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, attn_mask):\n",
    "        # apply attention over RNN outputs (batch, seq, hidden)\n",
    "        attn = torch.bmm(x, x.transpose(1, 2))\n",
    "        attn.data.masked_fill_(attn_mask.byte(), -float('inf'))\n",
    "        attn = torch.softmax(attn, dim=2)\n",
    "        # account for padding \n",
    "        attn.data.masked_fill_(attn_mask.byte(), 0)\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_Self_Attention(lang, device, batch_size, 25, 30, lang.n_words, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RNN_Self_Attention(\n",
       "   (embedding): Embedding(10004, 25)\n",
       "   (gru): GRU(25, 30)\n",
       "   (classify): Linear(in_features=60, out_features=1, bias=True)\n",
       "   (attention): RNNAttention()\n",
       " ),\n",
       " Embedding(10004, 25),\n",
       " GRU(25, 30),\n",
       " Linear(in_features=60, out_features=1, bias=True),\n",
       " RNNAttention()]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = model._modules.get('attention')\n",
    "tracked_attention_weigths = []\n",
    "def show(m, i, o): \n",
    "    w = o\n",
    "    tracked_attention_weigths.append(w)\n",
    "hook = attentions.register_forward_hook(show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(sents)\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tracked_attention_weigths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_index = 2\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "attention = w.detach().numpy().squeeze()\n",
    "attention = attention[batch_index] if type(batch_index) == int else attention\n",
    "ax.matshow(attention, cmap='viridis')\n",
    "fontdict = {'fontsize': 14}\n",
    "print(np.sum(attention[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Example from dataset\n",
    "batch_size = 3\n",
    "df = traindf[traindf.file_length < 50]\n",
    "max_sentence_len = 25\n",
    "for sents, targets in batch_iter(lang, df, batch_size, max_sentence_len, shuffle=True):\n",
    "    break\n",
    "max(map(len, sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sents[0]\n",
    "target = targets[0]\n",
    "task = 0 #'<sentiment>'\n",
    "task = torch.tensor([task] * batch_size)\n",
    "task.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, lengths = to_input_tensor(lang, sents, device)\n",
    "x = x.transpose(0, 1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 20\n",
    "w_embedding = nn.Embedding(lang.n_words, embed_dim)\n",
    "t_embedding = nn.Embedding(1, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = t_embedding(task).unsqueeze(-1)\n",
    "te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 30\n",
    "linear = nn.Linear(embed_dim, hidden_dim)\n",
    "classify = nn.Linear(hidden_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xe = w_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.3\n",
    "mha = nn.MultiheadAttention(embed_dim, 1, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 30])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xa, _ = mha(xe, xe, xe)\n",
    "x = linear(xa)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.bmm(x, te)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.3235e-06],\n",
       "        [2.1673e-07],\n",
       "        [1.1966e-03],\n",
       "        [2.6087e-04],\n",
       "        [3.0807e-03],\n",
       "        [1.4841e-07],\n",
       "        [7.4762e-01],\n",
       "        [2.8914e-06],\n",
       "        [5.5351e-03],\n",
       "        [5.8624e-07],\n",
       "        [2.2333e-04],\n",
       "        [1.6494e-01],\n",
       "        [2.8781e-06],\n",
       "        [7.7139e-02]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(w.squeeze(-1), -1).unsqueeze(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 30])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_attention = w * x\n",
    "weighted_attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 14])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wa = weighted_attention.transpose(1, 2)\n",
    "wa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.2636e-04],\n",
       "        [4.7082e-02],\n",
       "        [5.3003e-05]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxpool, _ = torch.max(wa, -1)\n",
    "y = torch.sigmoid(classify(maxpool))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated to Models after june3-851"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskSpecificAttention(nn.Module):\n",
    "    def __init__(self, language, device, embed_dim, hidden_dim, num_embed, num_heads, num_layers, dropout, n_classes):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.language = language\n",
    "        \n",
    "        self.w_embedding = nn.Embedding(lang.n_words, embed_dim)\n",
    "        self.t_embedding = nn.Embedding(num_layers, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.mhas, self.linear_1, self.linear_2 = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        self.ln_1, self.ln_2 = nn.ModuleList(), nn.ModuleList()\n",
    "        self.tasks = []\n",
    "        self.attention = TaskAttention()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.mhas.append(nn.MultiheadAttention(embed_dim, 1, dropout=dropout))\n",
    "            self.linear_1.append(nn.Linear(embed_dim, hidden_dim))\n",
    "            self.linear_2.append(nn.Linear(hidden_dim, embed_dim))\n",
    "            self.tasks.append(i)\n",
    "            \n",
    "            self.ln_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.ln_2.append(nn.LayerNorm(hidden_dim, eps=1e-12))\n",
    "        \n",
    "        self.classify = nn.Linear(embed_dim, n_classes)\n",
    "        \n",
    "    def forward(self, sents):\n",
    "        batch_size = len(sents)\n",
    "        x, lengths = to_input_tensor(lang, sents, device)\n",
    "        x = x.transpose(0, 1)\n",
    "        # bs, seq, embed\n",
    "        x = self.w_embedding(x)\n",
    "\n",
    "        for task, mha, linear_1, linear_2, lnorm_1, lnorm_2 in zip(self.tasks, self.mhas, self.linear_1, self.linear_2, self.ln_1, self.ln_2):\n",
    "            tasks = torch.tensor([task] * batch_size, device=self.device)\n",
    "            te = self.t_embedding(tasks).unsqueeze(-1)\n",
    "            \n",
    "            x = lnorm_1(x)\n",
    "            # bs, seq, embed\n",
    "            x, _ = mha(x, x, x)\n",
    "            # bs, seq, hidden\n",
    "            x = linear_1(x)\n",
    "            \n",
    "            # task attention\n",
    "            w = self.attention(x, te)\n",
    "            weighted_attention = w * x\n",
    "            x = self.dropout(weighted_attention)\n",
    "            \n",
    "            x = lnorm_2(x)\n",
    "            # bs, seq, embed\n",
    "            x = F.relu(linear_2(x))\n",
    "\n",
    "        # bs, embed, seq\n",
    "        x = x.transpose(1, 2)\n",
    "        maxpool, _ = torch.max(x, -1)\n",
    "        maxpool = bn\n",
    "        y = torch.sigmoid(self.classify(maxpool)).squeeze()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 1\n",
    "n_layers = 2\n",
    "dropout = 0.3 \n",
    "\n",
    "model = TaskSpecificAttention(lang, device, embed_dim, hidden_dim, lang.n_words, n_heads, n_layers, dropout, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskAttention(SaveModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, te):\n",
    "        # task attention\n",
    "        w = torch.bmm(x, te)\n",
    "        w = torch.softmax(w.squeeze(-1), -1).unsqueeze(-1)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Example from dataset\n",
    "batch_size = 3\n",
    "df = traindf[traindf.file_length < 50]\n",
    "max_sentence_len = 25\n",
    "for sents, targets in batch_iter(lang, df, batch_size, max_sentence_len, shuffle=True):\n",
    "    break\n",
    "max(map(len, sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = model._modules.get('attention')\n",
    "tracked_attention_weigths = []\n",
    "def show(m, i, o): \n",
    "    w = o\n",
    "    tracked_attention_weigths.append(w)\n",
    "hook = attentions.register_forward_hook(show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(sents)\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_attn = tracked_attention_weigths[0].detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "attn = batch_attn[i][np.newaxis, ...]\n",
    "sentence = sents[i]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.matshow(attn, cmap='viridis')\n",
    "ax.set_xticklabels([''] + sentence, rotation=30)\n",
    "# ax.set_yticklabels([''] + sentence)\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 24, 20])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = len(sents)\n",
    "x, lengths = to_input_tensor(lang, sents, device)\n",
    "x = x.transpose(0, 1)\n",
    "# bs, seq, embed\n",
    "x = w_embedding(x)\n",
    "x = x.unsqueeze(1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1, 1, 3, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 400])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((batch_size, embed_dim, 400))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 50])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.MaxPool1d(8)\n",
    "p = m(x)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = 100 - p.size(-1)\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 50])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad = torch.zeros((p.size(0), p.size(1), diff))\n",
    "pad.require_grad = False\n",
    "pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 100])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat([p, pad], -1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = nn.Linear(100, hidden_dim)\n",
    "h2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "h3 = nn.Linear(embed_dim, 1)\n",
    "classify = nn.Linear(hidden_dim, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = h1(x)\n",
    "x = h2(x)\n",
    "x = h3(x.transpose(-1, -2)).squeeze()\n",
    "x = classify(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2151],\n",
       "        [-0.2084],\n",
       "        [-0.2051]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one file, get `batch_size - 1` other files with file_length within a certain amount of it \n",
    "n = 10\n",
    "batch_size = 64\n",
    "file_lengths = traindf.file_length.values\n",
    "tmpdf = traindf.copy()\n",
    "\n",
    "for (p, t, _, length) in traindf.values:\n",
    "    lb, ub = length - n, length + n\n",
    "    # find files lb < _ < ub for file lengths\n",
    "    idxs = [i for i, fl in enumerate(file_lengths) if (fl >= lb and fl <= ub)]\n",
    "    \n",
    "    random.shuffle(idxs)\n",
    "    idxs = idxs[:batch_size] if len(idxs) > batch_size else idxs\n",
    "    batchdf = traindf.loc[idxs]\n",
    "    \n",
    "    tmpdf = traindf[~traindf.index.isin(batchdf.index)]\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-341-b0b624c61d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfile_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfl_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_lengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlb\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-341-b0b624c61d76>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfile_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfl_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_lengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlb\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "batch_size = 64\n",
    "file_lengths = traindf.file_length.values\n",
    "tmpdf = traindf.copy()\n",
    "\n",
    "while len(tmpdf) > batch_size:\n",
    "    (_, _, _, length) = tmpdf.values[0]\n",
    "    lb, ub = length - n, length + n\n",
    "    file_lengths = tmpdf.file_length.values\n",
    "    fl_idxs = tmpdf.file_length.index\n",
    "    idxs = [i for i, fl in zip(fl_idxs, file_lengths) if (fl >= lb and fl <= ub)]\n",
    "    \n",
    "    random.shuffle(idxs)\n",
    "    idxs = idxs[:batch_size] if len(idxs) > batch_size else idxs\n",
    "    batchdf = tmpdf.loc[idxs]\n",
    "    \n",
    "    # remove selected batch rows \n",
    "    tmpdf = tmpdf[~tmpdf.index.isin(batchdf.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    1,     2,     3,     4,     5,     7,     8,     9,    10,\n",
       "               11,\n",
       "            ...\n",
       "            24989, 24990, 24991, 24993, 24994, 24995, 24996, 24997, 24998,\n",
       "            24999],\n",
       "           dtype='int64', length=23005)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpdf.file_length.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10097, 9973)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmpdf), max(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>target</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>file_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train/neg/3351_4.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train/neg/399_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train/neg/10447_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train/neg/10096_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train/neg/9850_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     path  target  review_rating  file_length\n",
       "6    train/neg/3351_4.txt       0              4          197\n",
       "7     train/neg/399_2.txt       0              2           93\n",
       "8   train/neg/10447_1.txt       0              1          170\n",
       "9   train/neg/10096_1.txt       0              1          127\n",
       "10   train/neg/9850_1.txt       0              1          129"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([6177, 3236, 579, 6441, 9379, 3866, 9102, 9346, 1297, 1872], dtype='int64')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-332-89e150674654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/nlp/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/nlp/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot index with multidimensional key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/nlp/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1203\u001b[0m             \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             keyarr, indexer = self._get_listlike_indexer(key, axis,\n\u001b[0;32m-> 1205\u001b[0;31m                                                          raise_missing=False)\n\u001b[0m\u001b[1;32m   1206\u001b[0m             return self.obj._reindex_with_indexers({axis: [keyarr, indexer]},\n\u001b[1;32m   1207\u001b[0m                                                    copy=True, allow_dups=True)\n",
      "\u001b[0;32m~/miniconda2/envs/nlp/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/nlp/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([6177, 3236, 579, 6441, 9379, 3866, 9102, 9346, 1297, 1872], dtype='int64')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "tmpdf.loc[idxs[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(_, _, _, length) = traindf.values[0]\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(idxs)\n",
    "idxs = idxs[:batch_size] if len(idxs) > batch_size else idxs\n",
    "len(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>target</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>file_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11430</th>\n",
       "      <td>train/neg/6255_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11290</th>\n",
       "      <td>train/neg/3922_3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16213</th>\n",
       "      <td>train/pos/12193_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18001</th>\n",
       "      <td>train/pos/3884_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19857</th>\n",
       "      <td>train/pos/8292_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         path  target  review_rating  file_length\n",
       "11430    train/neg/6255_1.txt       0              1           49\n",
       "11290    train/neg/3922_3.txt       0              3           51\n",
       "16213  train/pos/12193_10.txt       1             10           40\n",
       "18001    train/pos/3884_8.txt       1              8           36\n",
       "19857    train/pos/8292_8.txt       1              8           42"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchdf = traindf.loc[idxs]\n",
    "batchdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindf[~traindf.index.isin(batchdf.index)]) == len(traindf) - batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "updateddf = traindf[~traindf.index.isin(batchdf.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz\n",
    "import pickle\n",
    "import numpy as np \n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'{glove_path}/glove.840B.300d.txt', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=f'{glove_path}/6B.50.dat', mode='w')\n",
    "\n",
    "with open(f'{glove_path}/glove.840B.300d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split(' ')\n",
    "        word = line[0]\n",
    "        if word not in words:\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        try:\n",
    "            vect = np.array(line[1:]).astype(np.float)\n",
    "        except:\n",
    "            print(line)\n",
    "            \n",
    "        if idx % 10000 == 0: print(idx)\n",
    "        vectors.append(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = bcolz.carray(vectors[1:].reshape((2196017, 300)), rootdir=f'{glove_path}/6B.50.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(f'{glove_path}/840B.300_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(f'{glove_path}/840B.300_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.175731897354126"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectors = bcolz.open(f'{glove_path}/6B.50.dat')[:]\n",
    "# words = pickle.load(open(f'{glove_path}/6B.50_words.pkl', 'rb'))\n",
    "# word2idx = pickle.load(open(f'{glove_path}/6B.50_idx.pkl', 'rb'))\n",
    "\n",
    "start = time.time()\n",
    "glove = {w: vectors[word2idx[w]] for w in words}\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "matrix_len = lang.n_words\n",
    "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "words_found = 0\n",
    "\n",
    "for i in lang.id2word:\n",
    "    word = lang.id2word[i]\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.798226135783563"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_found / lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((78360, 300), 78360)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_matrix.shape, lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./glove/imdb_weights.pkl', 'wb') as f:\n",
    "    weights_matrix.dump(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "matrix_len = lang.n_words\n",
    "with open('./glove/imdb_weights.pkl', 'rb') as f:\n",
    "    weights_matrix = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78360, 300)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_embeddings(trainable=False):\n",
    "    with open('./glove/imdb_weights.pkl', 'rb') as f:\n",
    "        weights_matrix = np.load(f, allow_pickle=True)\n",
    "    mtrx = torch.tensor(weights_matrix)\n",
    "    \n",
    "    embedding = nn.Embedding(mtrx.size(0), 300)\n",
    "    embedding.load_state_dict({'weight': mtrx})\n",
    "    \n",
    "    if not trainable:\n",
    "        embedding.requires_grad = False\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(78360, 300)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_structure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlang = lang.top_n_words_model(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tlang.word2id:\n",
    "    i = tlang.word2id[w]\n",
    "    assert i == lang.word2id[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchsize, seq_len, 1\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1268],\n",
       "        [0.1334],\n",
       "        [0.2253],\n",
       "        [0.2560],\n",
       "        [0.2586]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with seq_len values \n",
    "seq_len = 5 \n",
    "w = torch.rand((seq_len, 1))\n",
    "w = F.softmax(w.squeeze(), -1).unsqueeze(-1)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1897, 0.2059, 0.2004, 0.2031, 0.2009])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = F.softmax(w.squeeze(), -1)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.3342, 0.0000, 0.3333, 0.3325]), tensor([0, 2]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n indexs of smallest attention values\n",
    "restrict_idx = torch.argsort(w)[:n]\n",
    "w[restrict_idx] = -float('Inf')\n",
    "F.softmax(w, -1), restrict_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(w.squeeze(), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1268, 0.1334, 0.2253, 0.2560, 0.2586]),\n",
       " tensor([0.1318, 0.1252, 0.0333, 0.0026, 0.0000]),\n",
       " tensor([1, 0]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multinomial test\n",
    "# order by distance from the max value \n",
    "w = w.squeeze()\n",
    "# inverse probability hack for multinomial sampling\n",
    "p_inv = torch.max(w) - w\n",
    "w, p_inv, torch.multinomial(p_inv, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2093, 0.1619, 0.2502, 0.2380, 0.1405],\n",
       "        [0.2210, 0.1721, 0.2060, 0.1967, 0.2042]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BATCH VERSION\n",
    "# start with seq_len values \n",
    "seq_len = 5 \n",
    "w = torch.rand((2, seq_len, 1))\n",
    "w = F.softmax(w.squeeze(), -1)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0, 1, 4, 3],\n",
       "        [2, 3, 0, 1, 4]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restrict_idx = torch.argsort(w, -1)\n",
    "restrict_idx\n",
    "# w[restrict_idx] = -float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restrict_idx[:, :n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -inf, 0.2018,   -inf, 0.2701, 0.2047],\n",
       "        [0.1775, 0.1784,   -inf,   -inf, 0.3070]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply restricted attention mask\n",
    "for w_t, mask in zip(torch.split(w, 1), restrict_idx[:, :n]):\n",
    "    w_t.squeeze()[mask] = -float('Inf')\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.3254, 0.0000, 0.3483, 0.3263],\n",
       "        [0.3186, 0.3188, 0.0000, 0.0000, 0.3626]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(w, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1455, 0.2532, 0.1438, 0.1501, 0.3074],\n",
       "         [0.2107, 0.1732, 0.2231, 0.1592, 0.2339]]), torch.return_types.max(\n",
       " values=tensor([0.3074, 0.2339]),\n",
       " indices=tensor([4, 4])))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, torch.max(w, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 5]))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inverse probability hack for multinomial sampling\n",
    "mx, _ = torch.max(w, -1)\n",
    "mx = mx.unsqueeze(-1)\n",
    "mx.shape, w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2093, 0.1619, 0.2502, 0.2380, 0.1405],\n",
       "         [0.2210, 0.1721, 0.2060, 0.1967, 0.2042]]),\n",
       " tensor([[0.0409, 0.0883, 0.0000, 0.0122, 0.1097],\n",
       "         [0.0000, 0.0489, 0.0150, 0.0243, 0.0168]]))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_inv = mx - w\n",
    "w, p_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [1, 3]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attnmask = torch.multinomial(p_inv, n)\n",
    "attnmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -inf,   -inf, 0.2502, 0.2380, 0.1405],\n",
       "        [0.2210,   -inf, 0.2060,   -inf, 0.2042]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply restricted attention mask\n",
    "for w_t, mask in zip(torch.split(w, 1), attnmask):\n",
    "    w_t.squeeze()[mask] = -float('Inf')\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.3467, 0.3425, 0.3107],\n",
       "        [0.3369, 0.0000, 0.3319, 0.0000, 0.3313]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-weight with softmax\n",
    "attn = F.softmax(w, -1)\n",
    "attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working version - adjusted for training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2286, 0.1687, 0.1031, 0.2528, 0.2468],\n",
       "        [0.2310, 0.2020, 0.1548, 0.2269, 0.1853]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BATCH VERSION\n",
    "# start with seq_len values \n",
    "seq_len = 5 \n",
    "w = torch.rand((2, seq_len, 1))\n",
    "w = F.softmax(w.squeeze(), -1)\n",
    "\n",
    "inf = torch.tensor(float(\"inf\")).to(device)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [1, 4]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 where inf is applied\n",
    "n = w.size(-1) // 2\n",
    "# inverse probability hack for multinomial sampling\n",
    "mx, _ = torch.max(w, -1)\n",
    "mx = mx.unsqueeze(-1)\n",
    "p_inv = mx - w\n",
    "attnmask = torch.multinomial(p_inv, n)\n",
    "attnmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 1., 0., 0.],\n",
       "         [0., 1., 0., 0., 1.]]), tensor([[1, 2],\n",
       "         [1, 4]]))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_mask = torch.zeros_like(w)\n",
    "for bm, mask in zip(torch.split(byte_mask, 1), attnmask):\n",
    "    bm.squeeze()[mask] = 1\n",
    "byte_mask, attnmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 1, 1],\n",
       "        [1, 0, 1, 1, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_bytes = byte_mask.byte().to(device)\n",
    "1 - attn_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2055, 0.1935, 0.1812, 0.2105, 0.2092],\n",
       "        [0.2062, 0.2003, 0.1911, 0.2054, 0.1970]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(w, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchVision Dataset Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch \n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = torchvision.datasets.MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = dset(root='./data', train=True, download=False, transform=transform)\n",
    "testset = dset(root='./data', train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: ./data\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MNIST' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d670cec377b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0midb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataBunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0midb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/fastai/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(self, rows, ds_type, reverse, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mn_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_square_show\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_items\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrab_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MNIST' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "from fastai.vision import * \n",
    "idb = ImageDataBunch(trainloader, trainloader)\n",
    "idb.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "img, label = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 5, 6, 9]), torch.Size([4, 1, 28, 28]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label, img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npimg = img.numpy()[0].transpose(1, 2, 0).squeeze()\n",
    "npimg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(npimg)\n",
    "plt.title(label[0].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "\n",
    "import fastai\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.52'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = unpickle('./cifar-10-batches-py/data_batch_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dset(Dataset):\n",
    "    def __init__(self, data, labels, transforms):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.transforms(self.data[idx])\n",
    "        return img, self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databatch1 = d[b'data'].reshape((10000, -1, 32, 32))\n",
    "databatch1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dset(databatch1, d[b'labels'], transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(d, batch_size=4,\n",
    "                                      shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'fastai.vision.data' has no attribute 'ImageDataBunch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ab7d54204909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageDataBunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0midb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'fastai.vision.data' has no attribute 'ImageDataBunch'"
     ]
    }
   ],
   "source": [
    "idb = ImageDataBunch(trainloader, trainloader)\n",
    "idb.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = databatch1[2].transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ex)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(databatch1, batch_size=4,\n",
    "                                          shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4016aa7eff77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/fastai/lib/python3.6/site-packages/fastai/vision/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fns, labels, classes)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;34m\"Dataset for folders of images in style {folder}/{class}/{images}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFilePathList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mImgLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mClasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "db = ImageDataset(trainloader, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-3ee9be7320e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/fastai/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(self, rows, ds_type, reverse, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;34m\"Show a batch of data in `ds_type` on a few `rows`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mn_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_square_show\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/fastai/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, ds_type, detach, denorm, cpu)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "db.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
