{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import operator\n",
    "import concurrent\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import spacy\n",
    "import pickle\n",
    "import torch \n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from io import open\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import CONTRACTION_MAP\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "base = Path('../aclImdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = base/'train/neg'\n",
    "train_pos = base/'train/pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_exs = []\n",
    "neg_ratings = []\n",
    "for ex in train_neg.iterdir():\n",
    "    neg_exs.append('train/neg/' + ex.name)\n",
    "    neg_ratings.append(int(ex.name.split('_')[-1].split('.')[0]))\n",
    "neg_labels = [0] * len(neg_exs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_exs = []\n",
    "pos_ratings = []\n",
    "for ex in train_pos.iterdir():\n",
    "    pos_exs.append('train/pos/' + ex.name)\n",
    "    pos_ratings.append(int(ex.name.split('_')[-1].split('.')[0]))\n",
    "pos_labels = [1] * len(pos_exs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'path': neg_exs + pos_exs, 'target': neg_labels + pos_labels, 'review_rating': neg_ratings + pos_ratings})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[id]_[rating].txt \n",
    "- where [id] is a unique id and [rating] is the star **rating for that review** on a 1-10 scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>target</th>\n",
       "      <th>review_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>train/neg/1573_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>train/neg/6298_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12304</th>\n",
       "      <td>train/neg/4670_4.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>train/neg/5154_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8327</th>\n",
       "      <td>train/neg/12485_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        path  target  review_rating\n",
       "228     train/neg/1573_1.txt       0              1\n",
       "4261    train/neg/6298_2.txt       0              2\n",
       "12304   train/neg/4670_4.txt       0              4\n",
       "4455    train/neg/5154_1.txt       0              1\n",
       "8327   train/neg/12485_1.txt       0              1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(frac=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "neg_df = df[df['target'] == 0]\n",
    "pos_df = df[df['target'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>target</th>\n",
       "      <th>review_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16634</th>\n",
       "      <td>train/pos/3602_7.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12573</th>\n",
       "      <td>train/pos/5708_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21552</th>\n",
       "      <td>train/pos/11873_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7495</th>\n",
       "      <td>train/neg/5024_4.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9868</th>\n",
       "      <td>train/neg/3804_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        path  target  review_rating\n",
       "16634   train/pos/3602_7.txt       1              7\n",
       "12573  train/pos/5708_10.txt       1             10\n",
       "21552  train/pos/11873_8.txt       1              8\n",
       "7495    train/neg/5024_4.txt       0              4\n",
       "9868    train/neg/3804_2.txt       0              2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(frac=1.).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = df.loc[1]\n",
    "path = ex.path\n",
    "target = ex.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file removing trailing spaces\n",
    "file = open(str(base/path), encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. however, they proceeded to make tremors II and III. Trust me, those movies started going downhill right after they finished the first one, i mean, ass blasters??? Now, only God himself is capable of answering the question \"why in Gods name would they create another one of these dumpster dives of a movie?\" Tremors IV cannot be considered a bad movie, in fact it cannot be even considered an epitome of a bad movie, for it lives up to more than that. As i attempted to sit though it, i noticed that my eyes started to bleed, and i hoped profusely that the little girl from the ring would crawl through the TV and kill me. did they really think that dressing the people who had stared in the other movies up as though they we\\'re from the wild west would make the movie (with the exact same occurrences) any better? honestly, i would never suggest buying this movie, i mean, there are cheaper ways to find things that burn well.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(s):\n",
    "    soup = BeautifulSoup(s, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Expand contractions (it's = it is), thanks to \n",
    "# https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "# again, thanks to \n",
    "# https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def normalizeString(s, stopwords=True, contractions=False):\n",
    "    # Remove html tags \n",
    "    s = strip_html_tags(s.lower().strip())\n",
    "    # Lowercase, trim, and remove non-letter characters\n",
    "    s = unicodeToAscii(s)\n",
    "    # add spaces too ! ? .\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1 \", s)\n",
    "    # expand contractions \n",
    "    if not contractions:\n",
    "        s = expand_contractions(s)\n",
    "    # remove all other characters\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s).strip()\n",
    "    # remove stop words \n",
    "    if not stopwords: \n",
    "        s = remove_stopwords(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well . . . tremors i the original started off in and i found the movie quite enjoyable to watch . however they proceeded to make tremors ii and iii . trust me those movies started going downhill right after they finished the first one i mean ass blasters ? ? ? now only god himself is capable of answering the question why in gods name would they create another one of these dumpster dives of a movie ? tremors iv cannot be considered a bad movie in fact it cannot be even considered an epitome of a bad movie for it lives up to more than that . as i attempted to sit though it i noticed that my eyes started to bleed and i hoped profusely that the little girl from the ring would crawl through the tv and kill me . did they really think that dressing the people who had stared in the other movies up as though they we are from the wild west would make the movie with the exact same occurrences any better ? honestly i would never suggest buying this movie i mean there are cheaper ways to find things that burn well .'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_file = normalizeString(file, stopwords=True, contractions=False)\n",
    "clean_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to, \n",
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# stanford nlp a4\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2id = dict()\n",
    "        self.word2count = dict()\n",
    "        self.id2word = dict()\n",
    "        self.word2id['<pad>'] = 0   # Pad Token\n",
    "        self.word2id['<s>'] = 1     # Start Token\n",
    "        self.word2id['</s>'] = 2    # End Token\n",
    "        self.word2id['<unk>'] = 3   # Unknown Token\n",
    "        self.fixed_vocab = {'<pad>', '<s>' , '</s>', '<unk>'}\n",
    "        \n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.n_words = len(self.id2word)\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2id:\n",
    "            self.word2id[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.id2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def top_n_words_model(self, n):\n",
    "        top_lang = Lang()\n",
    "        ordered_words = sorted(lang.word2count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        for w, f in ordered_words[:n]: \n",
    "            top_lang.addWord(w)\n",
    "            top_lang.word2count[w] = f \n",
    "        return top_lang\n",
    "    \n",
    "    def get_id(self, word):\n",
    "        return self.word2id[word] if word in self.word2id else self.word2id['<unk>']\n",
    "            \n",
    "def dump_model(lang, name='imdb_language_class'):\n",
    "    lang_pkl = pickle.dumps(lang, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    open('{}.pkl'.format(name), 'wb').write(lang_pkl)\n",
    "    \n",
    "def load_model(name='imdb_language_class'):\n",
    "    with open('imdb_language_class.pkl', 'rb') as fp:\n",
    "        lang = pickle.load(fp)\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_track(lang, path):\n",
    "    file = open(str(base/path), encoding='utf-8').read()\n",
    "    # normalize\n",
    "    clean_file = normalizeString(file, stopwords=True, contractions=False)\n",
    "    # track words into model\n",
    "    for w in clean_file.split(' '):\n",
    "        lang.addWord(w)   \n",
    "    return True\n",
    "\n",
    "def populate_language(lang, df):\n",
    "    # Multithread normalizing and tracking the train dataset\n",
    "    start_time = time.time()\n",
    "    results = [normalize_and_track(lang, p) for p in df['path'].values]\n",
    "    duration = time.time() - start_time\n",
    "    print(\"Normalized and Tracked in {} seconds\".format(duration))\n",
    "    # Ensure success on all path values\n",
    "    assert all(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized and Tracked in 82.86931920051575 seconds\n"
     ]
    }
   ],
   "source": [
    "lang = Lang()\n",
    "populate_language(lang, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_model(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78360, 78360, 78360)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lang.word2id.items()), lang.n_words, len(lang.word2count.keys())+4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'41 27 27 27 42 43 8 44 45 46 47 34 43 48 8 49 50 51 15 52 27 53 54 55 15 56 42 57 34 58 27 59 60 61 62 45 63 64 65 66 54 67 8 68 6 43 69 70 71 72 72 72 73 74 75 76 19 77 7 78 8 79 80 47 81 82 83 54 84 85 6 7 86 87 88 7 24 49 72 42 89 90 16 91 24 92 49 47 93 18 90 16 94 91 95 96 7 24 92 49 97 18 98 99 15 100 101 102 27 103 43 104 15 105 106 18 43 107 102 108 109 45 15 110 34 43 111 112 102 8 113 114 30 8 115 83 116 117 8 118 34 119 60 27 120 54 121 122 102 123 8 124 125 126 127 47 8 128 62 99 103 106 54 129 130 30 8 131 132 83 56 8 49 5 8 133 134 135 136 137 72 138 43 83 139 140 141 12 49 43 69 35 130 142 143 15 144 145 102 146 41 27'"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_file = [lang.get_id(w) for w in clean_file.split(' ')]\n",
    "' '.join(list(map(str, int_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check word indexing \n",
    "assert \" \".join([lang.id2word[w_idx] for w_idx in int_file]) == clean_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_clean(lang, path):\n",
    "    file = open(str(base/path), encoding='utf-8').read()\n",
    "    clean_file = normalizeString(file, stopwords=True, contractions=False)\n",
    "    return clean_file\n",
    "\n",
    "def prepare_df(lang, df):\n",
    "    results = [(open_and_clean(lang, p), t) for (p, t) in zip(df['path'].values, df['target'].values)] \n",
    "    return results\n",
    "\n",
    "def batch_iter(lang, data, batch_size, shuffle=False):\n",
    "    batch_num = math.ceil(len(data) / batch_size)\n",
    "\n",
    "    if shuffle:\n",
    "        data = data.sample(frac=1.)\n",
    "    \n",
    "    for i in range(batch_num):\n",
    "        lb, ub = i * batch_size, min((i + 1) * batch_size, len(data))\n",
    "        batch_df = data[lb:ub]\n",
    "        \n",
    "        # open, clean, sort the batch_df\n",
    "        results = prepare_df(lang, batch_df)\n",
    "        results = sorted(results, key=lambda e: len(e[0].split(' ')), reverse=True)\n",
    "        sents, targets = [e[0].split(' ') for e in results], [e[1] for e in results]\n",
    "        \n",
    "        yield sents, torch.tensor(targets, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2id['<s>']] + [lang.get_id(word) for word in sentence] + [lang.word2id['</s>']]\n",
    "\n",
    "def to_input_tensor(lang, sents):\n",
    "    sents_id = [indexesFromSentence(lang, s) for s in sents]\n",
    "    lengths = [len(s) for s in sents_id]\n",
    "    sents_pad = pad_sents(sents_id, lang.word2id['<pad>'])\n",
    "    sents_var = torch.tensor(sents_pad, dtype=torch.long, device=device)\n",
    "    return torch.t(sents_var), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sents(sents, pad_token):\n",
    "    sents_padded = []\n",
    "    max_seq = max(map(len, sents))\n",
    "    def pad(s):\n",
    "        diff = max_seq - len(s)\n",
    "        s = s + [pad_token for _ in range(diff)]\n",
    "        return s\n",
    "    sents_padded = [pad(s) for s in sents]\n",
    "    return sents_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, lang):\n",
    "        super(SentModel, self).__init__()\n",
    "        self.lang = lang\n",
    "        self.embed = nn.Embedding(len(lang.word2id), embed_size, padding_idx=lang.word2id['<pad>'])\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, bias=True)\n",
    "        self.l1 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, sents):\n",
    "        s_tensor, lengths = to_input_tensor(self.lang, sents)\n",
    "        emb = self.embed(s_tensor)\n",
    "        \n",
    "        # pack + rnn sequence + unpack\n",
    "        x = nn.utils.rnn.pack_padded_sequence(emb, lengths)\n",
    "        output, hidden = self.gru(x)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output)\n",
    "        \n",
    "        # batch_size, seq_len, hidden_size\n",
    "        output_batch = output.transpose(0, 1)\n",
    "        \n",
    "        # batch_size, hidden_size\n",
    "        out_avg = output_batch.sum(dim=1)\n",
    "        \n",
    "        # batch_size, 1\n",
    "        linear_out = self.l1(out_avg)\n",
    "        out = torch.sigmoid(linear_out).squeeze(-1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 20\n",
    "embed_size = 300\n",
    "model = SentModel(embed_size, hidden_size, lang)\n",
    "model = model.to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "clip_grad = 5.\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fcn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-372-5186e28a040e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/nlp/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/nlp/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_batch_size = 32\n",
    "epochs = 2 \n",
    "\n",
    "for e in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    train_iter = -1\n",
    "    begin_time = time.time()\n",
    "    \n",
    "    for sents, targets in batch_iter(lang, df, train_batch_size, shuffle=True):\n",
    "        train_iter += 1 \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        preds = model(sents)\n",
    "        loss = loss_fcn(preds, targets)\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if train_iter > 20: break\n",
    "    \n",
    "    print('epoch %d, avg. loss %.2f, time elapsed %.2f sec' % (epoch, \n",
    "                                                                epoch_loss / train_iter,\n",
    "                                                                time.time() - begin_time), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
