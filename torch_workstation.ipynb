{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from language_structure import *\n",
    "from train import batch_iter, load\n",
    "from model import *\n",
    "\n",
    "base = Path('../aclImdb')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "traindf = pd.read_csv('train.csv')\n",
    "lang = load_model()\n",
    "lang = lang.top_n_words_model(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import to_input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Example from dataset\n",
    "batch_size = 3\n",
    "df = traindf[traindf.file_length < 25]\n",
    "for sents, targets in batch_iter(lang, df, batch_size, shuffle=True):\n",
    "    break\n",
    "max(map(len, sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this movie will always be a broadway and movie classic as long as there are still people who sing dance and act .'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([lang.id2word[d] for d in [lang.get_id(w) for w in sents[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed, embed_size = lang.n_words, 10\n",
    "embedding = nn.Embedding(n_embed, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3, 10])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, lengths = to_input_tensor(lang, sents, device)\n",
    "em = embedding(x)\n",
    "em.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as usual sean connery does a great job . lawrence fishburn is good but i have a hard time not seeing him as ike turner .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 3\n",
    "gru = nn.GRU(embed_size, hidden_size, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3, 3])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nn.utils.rnn.pack_padded_sequence(em, lengths)\n",
    "output, hidden = gru(x)\n",
    "output, _ = nn.utils.rnn.pad_packed_sequence(output)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.transpose(0, 1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_attn_mask = torch.ones_like(sent_encoded)\n",
    "old_attn_mask[0, :] = -float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 26, 26]), torch.Size([3, 26, 3]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape, sent_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_vec = attn.unsqueeze(-1) * sent_encoded\n",
    "attn_vec = torch.sum(attn_vec, 1)\n",
    "attn_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([23, 6]),\n",
       " tensor([-0.0985, -0.1809,  0.0910, -0.5316, -0.2783,  0.4990],\n",
       "        grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hidden = torch.cat([attn_vec, sent_encoded], dim=1)\n",
    "total_hidden.shape, total_hidden[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0106,  0.0033,  0.0150],\n",
       "         [ 0.0165,  0.0026,  0.0247],\n",
       "         [ 0.0221,  0.0076,  0.0154],\n",
       "         [ 0.0134,  0.0133,  0.0043],\n",
       "         [ 0.0035, -0.0250, -0.0034],\n",
       "         [ 0.0163, -0.0103, -0.0057],\n",
       "         [ 0.0052, -0.0023,  0.0185],\n",
       "         [ 0.0243, -0.0045,  0.0175],\n",
       "         [ 0.0117, -0.0337,  0.0093],\n",
       "         [ 0.0032,  0.0010,  0.0072],\n",
       "         [ 0.0133,  0.0166, -0.0025],\n",
       "         [ 0.0139,  0.0137, -0.0042],\n",
       "         [-0.0316,  0.0124, -0.0029],\n",
       "         [-0.0768, -0.0651,  0.0015],\n",
       "         [-0.0375, -0.0340,  0.0034],\n",
       "         [-0.0180, -0.0215, -0.0003],\n",
       "         [-0.0614, -0.0607, -0.0004],\n",
       "         [ 0.0076, -0.0115, -0.0035],\n",
       "         [ 0.0223, -0.0049, -0.0006],\n",
       "         [-0.0208,  0.0176,  0.0010],\n",
       "         [-0.0242,  0.0325,  0.0051],\n",
       "         [-0.0120, -0.0280, -0.0086]], grad_fn=<MulBackward0>),\n",
       " torch.Size([23, 3]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.unsqueeze(-1) * sent_encoded, sent_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_vec = torch.sum(attn.unsqueeze(-1) * sent_encoded, dim=0)\n",
    "attn_vec.shape # hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0985, -0.1809,  0.0910, -0.5316, -0.2783,  0.4990],\n",
       "        grad_fn=<CatBackward>), torch.Size([6]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hidden = torch.cat([attn_vec, word_encoded.squeeze()])\n",
    "total_hidden, total_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 1\n",
    "lin = nn.Linear(2 * hidden_size, n_classes)\n",
    "# torch.sigmoid(lin(total_hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Self-Attention RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_encoded = output.transpose(0, 1)[0]\n",
    "\n",
    "# Make Attention Mask\n",
    "attn_mask = torch.eye(max(lengths)) * -float('Inf')\n",
    "attn_mask[torch.isnan(attn_mask)] = 0\n",
    "attn_mask[attn_mask == 0] = 1\n",
    "\n",
    "# Attention MM + Softmax\n",
    "attn = torch.mm(sent_encoded, sent_encoded.transpose(0, 1))\n",
    "attn = torch.softmax(attn * attn_mask, dim=1)\n",
    "\n",
    "# Sum along \n",
    "attn_vec = attn.unsqueeze(-1) * sent_encoded\n",
    "attn_vec = torch.sum(attn_vec, 1)\n",
    "\n",
    "total_hidden_single = torch.cat([attn_vec, sent_encoded], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Self-Attention RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 24, 13]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, lengths = to_input_tensor(lang, sents, device)\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 1., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = torch.eye(max(lengths))\n",
    "attn_mask = torch.stack([I] * batch_size)\n",
    "for i, l in zip(list(range(batch_size)), lengths):\n",
    "    attn_mask[i, :, l:] = 1\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, dtype=torch.uint8)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_encoded = output.transpose(0, 1)\n",
    "\n",
    "attn_mask = torch.eye(max(lengths))\n",
    "\n",
    "attn = torch.bmm(sent_encoded, sent_encoded.transpose(1, 2))\n",
    "attn.data.masked_fill_(attn_mask.byte(), -float('inf'))\n",
    "\n",
    "attn = torch.softmax(attn, dim=2)\n",
    "attn[torch.isnan(attn)] = 0\n",
    "\n",
    "attn_vec = attn.unsqueeze(-1) * sent_encoded.unsqueeze(1)\n",
    "attn_vec = attn_vec.sum(-2)\n",
    "\n",
    "total_hidden = torch.cat([attn_vec, sent_encoded], dim=-1)\n",
    "(total_hidden[0] == total_hidden_single).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 26])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hidden = total_hidden.transpose(-1, -2)\n",
    "total_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_hidden, idx = torch.max(total_hidden, -1)\n",
    "max_hidden.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAttention(nn.Module):\n",
    "    def __init__(self, language, device, embed_dim, hidden_dim, num_embed, n_classes):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.language = language \n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embed, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, bias=True)\n",
    "        self.classify = nn.Linear(2 * hidden_dim, n_classes)\n",
    "        \n",
    "    def forward(self, sents):\n",
    "        # Embed the sequence\n",
    "        x, lengths = to_input_tensor(self.language, sents, self.device)\n",
    "        x_embed = self.embedding(x)\n",
    "        # RNN encoding\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x_embed, lengths)\n",
    "        x, _ = self.gru(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x)\n",
    "\n",
    "        # batch, seq, hidden\n",
    "        x = x.transpose(0, 1)\n",
    "        # attention mask \n",
    "        attn_mask = torch.eye(max(lengths)) * -float('Inf')\n",
    "        attn_mask[torch.isnan(attn_mask)] = 0\n",
    "        attn_mask[attn_mask == 0] = 1\n",
    "\n",
    "        # apply attention over RNN outputs\n",
    "        attn = torch.bmm(x, x.transpose(1, 2))\n",
    "        attn = torch.softmax(attn * attn_mask, dim=2)\n",
    "        attn[torch.isnan(attn)] = 0\n",
    "        attn_vec = attn.unsqueeze(-1) * x.unsqueeze(1)\n",
    "        attn_vec = attn_vec.sum(-2)\n",
    "        attn_out = torch.cat([attn_vec, x], dim=-1)\n",
    "        \n",
    "        # max pool over sequence \n",
    "        attn_out = attn_out.transpose(-1, -2)\n",
    "        max_vec, _ = torch.max(attn_out, -1)\n",
    "        max_vec = max_vec.unsqueeze(-2)\n",
    "        \n",
    "        # binary classification activ.\n",
    "        y = torch.sigmoid(self.classify(max_vec)).squeeze()\n",
    "        return y        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Self_Attention(SaveModel):\n",
    "    def __init__(self, language, device, batch_size, embed_dim, hidden_dim, num_embed, n_classes):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.language = language \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embed, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, bias=True)\n",
    "        self.classify = nn.Linear(2 * hidden_dim, n_classes)\n",
    "        self.attention = RNNAttention()\n",
    "        \n",
    "    def forward(self, sents):\n",
    "        # Embed the sequence\n",
    "        x, lengths = to_input_tensor(self.language, sents, self.device)\n",
    "        x_embed = self.embedding(x)\n",
    "        # RNN encoding\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x_embed, lengths)\n",
    "        x, _ = self.gru(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x)\n",
    "        \n",
    "        x = x.transpose(0, 1)\n",
    "        # get attention over RNN outputs \n",
    "        I = torch.eye(max(lengths))\n",
    "        attn_mask = torch.stack([I] * self.batch_size)\n",
    "        for i, l in zip(list(range(self.batch_size)), lengths):\n",
    "            attn_mask[i, :, l:] = 1\n",
    "            attn_mask[i, l:, :] = 1\n",
    "        \n",
    "        attn = self.attention(x, attn_mask)\n",
    "        attn_vec = attn.unsqueeze(-1) * x.unsqueeze(1)\n",
    "        attn_vec = attn_vec.sum(-2)\n",
    "        attn_out = torch.cat([attn_vec, x], dim=-1)\n",
    "        \n",
    "        # max pool over sequence \n",
    "        attn_out = attn_out.transpose(-1, -2)\n",
    "        max_vec, _ = torch.max(attn_out, -1)\n",
    "        max_vec = max_vec.unsqueeze(-2)\n",
    "        \n",
    "        # binary classification activ.\n",
    "        y = torch.sigmoid(self.classify(max_vec)).squeeze()\n",
    "        return y        \n",
    "    \n",
    "class RNNAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, attn_mask):\n",
    "        # apply attention over RNN outputs (batch, seq, hidden)\n",
    "        attn = torch.bmm(x, x.transpose(1, 2))\n",
    "        attn.data.masked_fill_(attn_mask.byte(), -float('inf'))\n",
    "        attn = torch.softmax(attn, dim=2)\n",
    "        # account for padding \n",
    "        attn.data.masked_fill_(attn_mask.byte(), 0)\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_Self_Attention(lang, device, batch_size, 25, 30, lang.n_words, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RNN_Self_Attention(\n",
       "   (embedding): Embedding(10004, 25)\n",
       "   (gru): GRU(25, 30)\n",
       "   (classify): Linear(in_features=60, out_features=1, bias=True)\n",
       "   (attention): RNNAttention()\n",
       " ),\n",
       " Embedding(10004, 25),\n",
       " GRU(25, 30),\n",
       " Linear(in_features=60, out_features=1, bias=True),\n",
       " RNNAttention()]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = model._modules.get('attention')\n",
    "tracked_attention_weigths = []\n",
    "def show(m, i, o): \n",
    "    w = o\n",
    "    tracked_attention_weigths.append(w)\n",
    "hook = attentions.register_forward_hook(show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(sents)\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tracked_attention_weigths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_index = 2\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "attention = w.detach().numpy().squeeze()\n",
    "attention = attention[batch_index] if type(batch_index) == int else attention\n",
    "ax.matshow(attention, cmap='viridis')\n",
    "fontdict = {'fontsize': 14}\n",
    "print(np.sum(attention[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Example from dataset\n",
    "batch_size = 3\n",
    "df = traindf[traindf.file_length < 50]\n",
    "max_sentence_len = 25\n",
    "for sents, targets in batch_iter(lang, df, batch_size, max_sentence_len, shuffle=True):\n",
    "    break\n",
    "max(map(len, sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sents[0]\n",
    "target = targets[0]\n",
    "task = 0 #'<sentiment>'\n",
    "task = torch.tensor([task] * batch_size)\n",
    "task.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, lengths = to_input_tensor(lang, sents, device)\n",
    "x = x.transpose(0, 1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 20\n",
    "w_embedding = nn.Embedding(lang.n_words, embed_dim)\n",
    "t_embedding = nn.Embedding(1, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = t_embedding(task).unsqueeze(-1)\n",
    "te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 30\n",
    "linear = nn.Linear(embed_dim, hidden_dim)\n",
    "classify = nn.Linear(hidden_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xe = w_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.3\n",
    "mha = nn.MultiheadAttention(embed_dim, 1, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 30])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xa, _ = mha(xe, xe, xe)\n",
    "x = linear(xa)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.bmm(x, te)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.3235e-06],\n",
       "        [2.1673e-07],\n",
       "        [1.1966e-03],\n",
       "        [2.6087e-04],\n",
       "        [3.0807e-03],\n",
       "        [1.4841e-07],\n",
       "        [7.4762e-01],\n",
       "        [2.8914e-06],\n",
       "        [5.5351e-03],\n",
       "        [5.8624e-07],\n",
       "        [2.2333e-04],\n",
       "        [1.6494e-01],\n",
       "        [2.8781e-06],\n",
       "        [7.7139e-02]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(w.squeeze(-1), -1).unsqueeze(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 30])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_attention = w * x\n",
    "weighted_attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 14])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wa = weighted_attention.transpose(1, 2)\n",
    "wa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.2636e-04],\n",
       "        [4.7082e-02],\n",
       "        [5.3003e-05]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxpool, _ = torch.max(wa, -1)\n",
    "y = torch.sigmoid(classify(maxpool))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated to Models after june3-851"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskSpecificAttention(nn.Module):\n",
    "    def __init__(self, language, device, embed_dim, hidden_dim, num_embed, num_heads, num_layers, dropout, n_classes):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.language = language\n",
    "        \n",
    "        self.w_embedding = nn.Embedding(lang.n_words, embed_dim)\n",
    "        self.t_embedding = nn.Embedding(num_layers, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.mhas, self.linear_1, self.linear_2 = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        self.ln_1, self.ln_2 = nn.ModuleList(), nn.ModuleList()\n",
    "        self.tasks = []\n",
    "        self.attention = TaskAttention()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.mhas.append(nn.MultiheadAttention(embed_dim, 1, dropout=dropout))\n",
    "            self.linear_1.append(nn.Linear(embed_dim, hidden_dim))\n",
    "            self.linear_2.append(nn.Linear(hidden_dim, embed_dim))\n",
    "            self.tasks.append(i)\n",
    "            \n",
    "            self.ln_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.ln_2.append(nn.LayerNorm(hidden_dim, eps=1e-12))\n",
    "        \n",
    "        self.classify = nn.Linear(embed_dim, n_classes)\n",
    "        \n",
    "    def forward(self, sents):\n",
    "        batch_size = len(sents)\n",
    "        x, lengths = to_input_tensor(lang, sents, device)\n",
    "        x = x.transpose(0, 1)\n",
    "        # bs, seq, embed\n",
    "        x = self.w_embedding(x)\n",
    "\n",
    "        for task, mha, linear_1, linear_2, lnorm_1, lnorm_2 in zip(self.tasks, self.mhas, self.linear_1, self.linear_2, self.ln_1, self.ln_2):\n",
    "            tasks = torch.tensor([task] * batch_size, device=self.device)\n",
    "            te = self.t_embedding(tasks).unsqueeze(-1)\n",
    "            \n",
    "            x = lnorm_1(x)\n",
    "            # bs, seq, embed\n",
    "            x, _ = mha(x, x, x)\n",
    "            # bs, seq, hidden\n",
    "            x = linear_1(x)\n",
    "            \n",
    "            # task attention\n",
    "            w = self.attention(x, te)\n",
    "            weighted_attention = w * x\n",
    "            x = self.dropout(weighted_attention)\n",
    "            \n",
    "            x = lnorm_2(x)\n",
    "            # bs, seq, embed\n",
    "            x = F.relu(linear_2(x))\n",
    "\n",
    "        # bs, embed, seq\n",
    "        x = x.transpose(1, 2)\n",
    "        maxpool, _ = torch.max(x, -1)\n",
    "        maxpool = bn\n",
    "        y = torch.sigmoid(self.classify(maxpool)).squeeze()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 1\n",
    "n_layers = 2\n",
    "dropout = 0.3 \n",
    "\n",
    "model = TaskSpecificAttention(lang, device, embed_dim, hidden_dim, lang.n_words, n_heads, n_layers, dropout, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskAttention(SaveModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, te):\n",
    "        # task attention\n",
    "        w = torch.bmm(x, te)\n",
    "        w = torch.softmax(w.squeeze(-1), -1).unsqueeze(-1)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Example from dataset\n",
    "batch_size = 3\n",
    "df = traindf[traindf.file_length < 50]\n",
    "max_sentence_len = 25\n",
    "for sents, targets in batch_iter(lang, df, batch_size, max_sentence_len, shuffle=True):\n",
    "    break\n",
    "max(map(len, sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = model._modules.get('attention')\n",
    "tracked_attention_weigths = []\n",
    "def show(m, i, o): \n",
    "    w = o\n",
    "    tracked_attention_weigths.append(w)\n",
    "hook = attentions.register_forward_hook(show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(sents)\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_attn = tracked_attention_weigths[0].detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "attn = batch_attn[i][np.newaxis, ...]\n",
    "sentence = sents[i]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.matshow(attn, cmap='viridis')\n",
    "ax.set_xticklabels([''] + sentence, rotation=30)\n",
    "# ax.set_yticklabels([''] + sentence)\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 24, 20])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = len(sents)\n",
    "x, lengths = to_input_tensor(lang, sents, device)\n",
    "x = x.transpose(0, 1)\n",
    "# bs, seq, embed\n",
    "x = w_embedding(x)\n",
    "x = x.unsqueeze(1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1, 1, 3, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 400])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((batch_size, embed_dim, 400))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 50])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.MaxPool1d(8)\n",
    "p = m(x)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = 100 - p.size(-1)\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 50])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad = torch.zeros((p.size(0), p.size(1), diff))\n",
    "pad.require_grad = False\n",
    "pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 100])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat([p, pad], -1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = nn.Linear(100, hidden_dim)\n",
    "h2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "h3 = nn.Linear(embed_dim, 1)\n",
    "classify = nn.Linear(hidden_dim, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = h1(x)\n",
    "x = h2(x)\n",
    "x = h3(x.transpose(-1, -2)).squeeze()\n",
    "x = classify(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2151],\n",
       "        [-0.2084],\n",
       "        [-0.2051]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one file, get `batch_size - 1` other files with file_length within a certain amount of it \n",
    "n = 10\n",
    "batch_size = 64\n",
    "file_lengths = traindf.file_length.values\n",
    "tmpdf = traindf.copy()\n",
    "\n",
    "for (p, t, _, length) in traindf.values:\n",
    "    lb, ub = length - n, length + n\n",
    "    # find files lb < _ < ub for file lengths\n",
    "    idxs = [i for i, fl in enumerate(file_lengths) if (fl >= lb and fl <= ub)]\n",
    "    \n",
    "    random.shuffle(idxs)\n",
    "    idxs = idxs[:batch_size] if len(idxs) > batch_size else idxs\n",
    "    batchdf = traindf.loc[idxs]\n",
    "    \n",
    "    tmpdf = traindf[~traindf.index.isin(batchdf.index)]\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-341-b0b624c61d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfile_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfl_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_lengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlb\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-341-b0b624c61d76>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfile_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfl_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_lengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlb\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "batch_size = 64\n",
    "file_lengths = traindf.file_length.values\n",
    "tmpdf = traindf.copy()\n",
    "\n",
    "while len(tmpdf) > batch_size:\n",
    "    (_, _, _, length) = tmpdf.values[0]\n",
    "    lb, ub = length - n, length + n\n",
    "    file_lengths = tmpdf.file_length.values\n",
    "    fl_idxs = tmpdf.file_length.index\n",
    "    idxs = [i for i, fl in zip(fl_idxs, file_lengths) if (fl >= lb and fl <= ub)]\n",
    "    \n",
    "    random.shuffle(idxs)\n",
    "    idxs = idxs[:batch_size] if len(idxs) > batch_size else idxs\n",
    "    batchdf = tmpdf.loc[idxs]\n",
    "    \n",
    "    # remove selected batch rows \n",
    "    tmpdf = tmpdf[~tmpdf.index.isin(batchdf.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    1,     2,     3,     4,     5,     7,     8,     9,    10,\n",
       "               11,\n",
       "            ...\n",
       "            24989, 24990, 24991, 24993, 24994, 24995, 24996, 24997, 24998,\n",
       "            24999],\n",
       "           dtype='int64', length=23005)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpdf.file_length.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10097, 9973)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmpdf), max(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>target</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>file_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train/neg/3351_4.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train/neg/399_2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train/neg/10447_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train/neg/10096_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train/neg/9850_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     path  target  review_rating  file_length\n",
       "6    train/neg/3351_4.txt       0              4          197\n",
       "7     train/neg/399_2.txt       0              2           93\n",
       "8   train/neg/10447_1.txt       0              1          170\n",
       "9   train/neg/10096_1.txt       0              1          127\n",
       "10   train/neg/9850_1.txt       0              1          129"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([6177, 3236, 579, 6441, 9379, 3866, 9102, 9346, 1297, 1872], dtype='int64')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-332-89e150674654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/nlp/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/nlp/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot index with multidimensional key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/nlp/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1203\u001b[0m             \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             keyarr, indexer = self._get_listlike_indexer(key, axis,\n\u001b[0;32m-> 1205\u001b[0;31m                                                          raise_missing=False)\n\u001b[0m\u001b[1;32m   1206\u001b[0m             return self.obj._reindex_with_indexers({axis: [keyarr, indexer]},\n\u001b[1;32m   1207\u001b[0m                                                    copy=True, allow_dups=True)\n",
      "\u001b[0;32m~/miniconda2/envs/nlp/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/nlp/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([6177, 3236, 579, 6441, 9379, 3866, 9102, 9346, 1297, 1872], dtype='int64')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "tmpdf.loc[idxs[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(_, _, _, length) = traindf.values[0]\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(idxs)\n",
    "idxs = idxs[:batch_size] if len(idxs) > batch_size else idxs\n",
    "len(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>target</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>file_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11430</th>\n",
       "      <td>train/neg/6255_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11290</th>\n",
       "      <td>train/neg/3922_3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16213</th>\n",
       "      <td>train/pos/12193_10.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18001</th>\n",
       "      <td>train/pos/3884_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19857</th>\n",
       "      <td>train/pos/8292_8.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         path  target  review_rating  file_length\n",
       "11430    train/neg/6255_1.txt       0              1           49\n",
       "11290    train/neg/3922_3.txt       0              3           51\n",
       "16213  train/pos/12193_10.txt       1             10           40\n",
       "18001    train/pos/3884_8.txt       1              8           36\n",
       "19857    train/pos/8292_8.txt       1              8           42"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchdf = traindf.loc[idxs]\n",
    "batchdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindf[~traindf.index.isin(batchdf.index)]) == len(traindf) - batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "updateddf = traindf[~traindf.index.isin(batchdf.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz\n",
    "import pickle\n",
    "import numpy as np \n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'{glove_path}/glove.840B.300d.txt', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=f'{glove_path}/6B.50.dat', mode='w')\n",
    "\n",
    "with open(f'{glove_path}/glove.840B.300d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split(' ')\n",
    "        word = line[0]\n",
    "        if word not in words:\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        try:\n",
    "            vect = np.array(line[1:]).astype(np.float)\n",
    "        except:\n",
    "            print(line)\n",
    "            \n",
    "        if idx % 10000 == 0: print(idx)\n",
    "        vectors.append(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = bcolz.carray(vectors[1:].reshape((2196017, 300)), rootdir=f'{glove_path}/6B.50.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(f'{glove_path}/840B.300_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(f'{glove_path}/840B.300_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.175731897354126"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectors = bcolz.open(f'{glove_path}/6B.50.dat')[:]\n",
    "# words = pickle.load(open(f'{glove_path}/6B.50_words.pkl', 'rb'))\n",
    "# word2idx = pickle.load(open(f'{glove_path}/6B.50_idx.pkl', 'rb'))\n",
    "\n",
    "start = time.time()\n",
    "glove = {w: vectors[word2idx[w]] for w in words}\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "matrix_len = lang.n_words\n",
    "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "words_found = 0\n",
    "\n",
    "for i in lang.id2word:\n",
    "    word = lang.id2word[i]\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.798226135783563"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_found / lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((78360, 300), 78360)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_matrix.shape, lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./glove/imdb_weights.pkl', 'wb') as f:\n",
    "    weights_matrix.dump(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "matrix_len = lang.n_words\n",
    "with open('./glove/imdb_weights.pkl', 'rb') as f:\n",
    "    weights_matrix = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78360, 300)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_embeddings(trainable=False):\n",
    "    with open('./glove/imdb_weights.pkl', 'rb') as f:\n",
    "        weights_matrix = np.load(f, allow_pickle=True)\n",
    "    mtrx = torch.tensor(weights_matrix)\n",
    "    \n",
    "    embedding = nn.Embedding(mtrx.size(0), 300)\n",
    "    embedding.load_state_dict({'weight': mtrx})\n",
    "    \n",
    "    if not trainable:\n",
    "        embedding.requires_grad = False\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(78360, 300)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_structure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlang = lang.top_n_words_model(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tlang.word2id:\n",
    "    i = tlang.word2id[w]\n",
    "    assert i == lang.word2id[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
