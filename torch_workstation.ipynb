{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from language_structure import *\n",
    "from train import batch_iter, load\n",
    "from model import *\n",
    "\n",
    "base = Path('../aclImdb')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "traindf = pd.read_csv('train.csv')\n",
    "lang = load_model()\n",
    "lang = lang.top_n_words_model(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import to_input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Example from dataset\n",
    "batch_size = 3\n",
    "df = traindf[traindf.file_length < 25]\n",
    "for sents, targets in batch_iter(lang, df, batch_size, shuffle=True):\n",
    "    break\n",
    "max(map(len, sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this movie will always be a broadway and movie classic as long as there are still people who sing dance and act .'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([lang.id2word[d] for d in [lang.get_id(w) for w in sents[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed, embed_size = lang.n_words, 10\n",
    "embedding = nn.Embedding(n_embed, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3, 10])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, lengths = to_input_tensor(lang, sents, device)\n",
    "em = embedding(x)\n",
    "em.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as usual sean connery does a great job . lawrence fishburn is good but i have a hard time not seeing him as ike turner .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 3\n",
    "gru = nn.GRU(embed_size, hidden_size, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3, 3])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nn.utils.rnn.pack_padded_sequence(em, lengths)\n",
    "output, hidden = gru(x)\n",
    "output, _ = nn.utils.rnn.pad_packed_sequence(output)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.transpose(0, 1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_attn_mask = torch.ones_like(sent_encoded)\n",
    "old_attn_mask[0, :] = -float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 26, 26]), torch.Size([3, 26, 3]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape, sent_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 3])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_vec = attn.unsqueeze(-1) * sent_encoded\n",
    "attn_vec = torch.sum(attn_vec, 1)\n",
    "attn_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([23, 6]),\n",
       " tensor([-0.0985, -0.1809,  0.0910, -0.5316, -0.2783,  0.4990],\n",
       "        grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hidden = torch.cat([attn_vec, sent_encoded], dim=1)\n",
    "total_hidden.shape, total_hidden[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0106,  0.0033,  0.0150],\n",
       "         [ 0.0165,  0.0026,  0.0247],\n",
       "         [ 0.0221,  0.0076,  0.0154],\n",
       "         [ 0.0134,  0.0133,  0.0043],\n",
       "         [ 0.0035, -0.0250, -0.0034],\n",
       "         [ 0.0163, -0.0103, -0.0057],\n",
       "         [ 0.0052, -0.0023,  0.0185],\n",
       "         [ 0.0243, -0.0045,  0.0175],\n",
       "         [ 0.0117, -0.0337,  0.0093],\n",
       "         [ 0.0032,  0.0010,  0.0072],\n",
       "         [ 0.0133,  0.0166, -0.0025],\n",
       "         [ 0.0139,  0.0137, -0.0042],\n",
       "         [-0.0316,  0.0124, -0.0029],\n",
       "         [-0.0768, -0.0651,  0.0015],\n",
       "         [-0.0375, -0.0340,  0.0034],\n",
       "         [-0.0180, -0.0215, -0.0003],\n",
       "         [-0.0614, -0.0607, -0.0004],\n",
       "         [ 0.0076, -0.0115, -0.0035],\n",
       "         [ 0.0223, -0.0049, -0.0006],\n",
       "         [-0.0208,  0.0176,  0.0010],\n",
       "         [-0.0242,  0.0325,  0.0051],\n",
       "         [-0.0120, -0.0280, -0.0086]], grad_fn=<MulBackward0>),\n",
       " torch.Size([23, 3]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.unsqueeze(-1) * sent_encoded, sent_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_vec = torch.sum(attn.unsqueeze(-1) * sent_encoded, dim=0)\n",
    "attn_vec.shape # hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0985, -0.1809,  0.0910, -0.5316, -0.2783,  0.4990],\n",
       "        grad_fn=<CatBackward>), torch.Size([6]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hidden = torch.cat([attn_vec, word_encoded.squeeze()])\n",
    "total_hidden, total_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 1\n",
    "lin = nn.Linear(2 * hidden_size, n_classes)\n",
    "# torch.sigmoid(lin(total_hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Self-Attention RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_encoded = output.transpose(0, 1)[0]\n",
    "\n",
    "# Make Attention Mask\n",
    "attn_mask = torch.eye(max(lengths)) * -float('Inf')\n",
    "attn_mask[torch.isnan(attn_mask)] = 0\n",
    "attn_mask[attn_mask == 0] = 1\n",
    "\n",
    "# Attention MM + Softmax\n",
    "attn = torch.mm(sent_encoded, sent_encoded.transpose(0, 1))\n",
    "attn = torch.softmax(attn * attn_mask, dim=1)\n",
    "\n",
    "# Sum along \n",
    "attn_vec = attn.unsqueeze(-1) * sent_encoded\n",
    "attn_vec = torch.sum(attn_vec, 1)\n",
    "\n",
    "total_hidden_single = torch.cat([attn_vec, sent_encoded], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Self-Attention RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 24, 13]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, lengths = to_input_tensor(lang, sents, device)\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 1., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = torch.eye(max(lengths))\n",
    "attn_mask = torch.stack([I] * batch_size)\n",
    "for i, l in zip(list(range(batch_size)), lengths):\n",
    "    attn_mask[i, :, l:] = 1\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, dtype=torch.uint8)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_encoded = output.transpose(0, 1)\n",
    "\n",
    "attn_mask = torch.eye(max(lengths))\n",
    "\n",
    "attn = torch.bmm(sent_encoded, sent_encoded.transpose(1, 2))\n",
    "attn.data.masked_fill_(attn_mask.byte(), -float('inf'))\n",
    "\n",
    "attn = torch.softmax(attn, dim=2)\n",
    "attn[torch.isnan(attn)] = 0\n",
    "\n",
    "attn_vec = attn.unsqueeze(-1) * sent_encoded.unsqueeze(1)\n",
    "attn_vec = attn_vec.sum(-2)\n",
    "\n",
    "total_hidden = torch.cat([attn_vec, sent_encoded], dim=-1)\n",
    "(total_hidden[0] == total_hidden_single).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 26])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hidden = total_hidden.transpose(-1, -2)\n",
    "total_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_hidden, idx = torch.max(total_hidden, -1)\n",
    "max_hidden.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAttention(nn.Module):\n",
    "    def __init__(self, language, device, embed_dim, hidden_dim, num_embed, n_classes):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.language = language \n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embed, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, bias=True)\n",
    "        self.classify = nn.Linear(2 * hidden_dim, n_classes)\n",
    "        \n",
    "    def forward(self, sents):\n",
    "        # Embed the sequence\n",
    "        x, lengths = to_input_tensor(self.language, sents, self.device)\n",
    "        x_embed = self.embedding(x)\n",
    "        # RNN encoding\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x_embed, lengths)\n",
    "        x, _ = self.gru(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x)\n",
    "\n",
    "        # batch, seq, hidden\n",
    "        x = x.transpose(0, 1)\n",
    "        # attention mask \n",
    "        attn_mask = torch.eye(max(lengths)) * -float('Inf')\n",
    "        attn_mask[torch.isnan(attn_mask)] = 0\n",
    "        attn_mask[attn_mask == 0] = 1\n",
    "\n",
    "        # apply attention over RNN outputs\n",
    "        attn = torch.bmm(x, x.transpose(1, 2))\n",
    "        attn = torch.softmax(attn * attn_mask, dim=2)\n",
    "        attn[torch.isnan(attn)] = 0\n",
    "        attn_vec = attn.unsqueeze(-1) * x.unsqueeze(1)\n",
    "        attn_vec = attn_vec.sum(-2)\n",
    "        attn_out = torch.cat([attn_vec, x], dim=-1)\n",
    "        \n",
    "        # max pool over sequence \n",
    "        attn_out = attn_out.transpose(-1, -2)\n",
    "        max_vec, _ = torch.max(attn_out, -1)\n",
    "        max_vec = max_vec.unsqueeze(-2)\n",
    "        \n",
    "        # binary classification activ.\n",
    "        y = torch.sigmoid(self.classify(max_vec)).squeeze()\n",
    "        return y        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Self_Attention(SaveModel):\n",
    "    def __init__(self, language, device, batch_size, embed_dim, hidden_dim, num_embed, n_classes):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.language = language \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embed, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, bias=True)\n",
    "        self.classify = nn.Linear(2 * hidden_dim, n_classes)\n",
    "        self.attention = RNNAttention()\n",
    "        \n",
    "    def forward(self, sents):\n",
    "        # Embed the sequence\n",
    "        x, lengths = to_input_tensor(self.language, sents, self.device)\n",
    "        x_embed = self.embedding(x)\n",
    "        # RNN encoding\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x_embed, lengths)\n",
    "        x, _ = self.gru(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x)\n",
    "        \n",
    "        x = x.transpose(0, 1)\n",
    "        # get attention over RNN outputs \n",
    "        I = torch.eye(max(lengths))\n",
    "        attn_mask = torch.stack([I] * self.batch_size)\n",
    "        for i, l in zip(list(range(self.batch_size)), lengths):\n",
    "            attn_mask[i, :, l:] = 1\n",
    "            attn_mask[i, l:, :] = 1\n",
    "        \n",
    "        attn = self.attention(x, attn_mask)\n",
    "        attn_vec = attn.unsqueeze(-1) * x.unsqueeze(1)\n",
    "        attn_vec = attn_vec.sum(-2)\n",
    "        attn_out = torch.cat([attn_vec, x], dim=-1)\n",
    "        \n",
    "        # max pool over sequence \n",
    "        attn_out = attn_out.transpose(-1, -2)\n",
    "        max_vec, _ = torch.max(attn_out, -1)\n",
    "        max_vec = max_vec.unsqueeze(-2)\n",
    "        \n",
    "        # binary classification activ.\n",
    "        y = torch.sigmoid(self.classify(max_vec)).squeeze()\n",
    "        return y        \n",
    "    \n",
    "class RNNAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, attn_mask):\n",
    "        # apply attention over RNN outputs (batch, seq, hidden)\n",
    "        attn = torch.bmm(x, x.transpose(1, 2))\n",
    "        attn.data.masked_fill_(attn_mask.byte(), -float('inf'))\n",
    "        attn = torch.softmax(attn, dim=2)\n",
    "        # account for padding \n",
    "        attn.data.masked_fill_(attn_mask.byte(), 0)\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_Self_Attention(lang, device, batch_size, 25, 30, lang.n_words, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RNN_Self_Attention(\n",
       "   (embedding): Embedding(10004, 25)\n",
       "   (gru): GRU(25, 30)\n",
       "   (classify): Linear(in_features=60, out_features=1, bias=True)\n",
       "   (attention): RNNAttention()\n",
       " ),\n",
       " Embedding(10004, 25),\n",
       " GRU(25, 30),\n",
       " Linear(in_features=60, out_features=1, bias=True),\n",
       " RNNAttention()]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = model._modules.get('attention')\n",
    "tracked_attention_weigths = []\n",
    "def show(m, i, o): \n",
    "    w = o\n",
    "    tracked_attention_weigths.append(w)\n",
    "hook = attentions.register_forward_hook(show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(sents)\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tracked_attention_weigths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_index = 2\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "attention = w.detach().numpy().squeeze()\n",
    "attention = attention[batch_index] if type(batch_index) == int else attention\n",
    "ax.matshow(attention, cmap='viridis')\n",
    "fontdict = {'fontsize': 14}\n",
    "print(np.sum(attention[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Example from dataset\n",
    "batch_size = 3\n",
    "df = traindf[traindf.file_length < 25]\n",
    "max_sentence_len = 25\n",
    "for sents, targets in batch_iter(lang, df, batch_size, max_sentence_len, shuffle=True):\n",
    "    break\n",
    "max(map(len, sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sents[0]\n",
    "target = targets[0]\n",
    "task = 0 #'<sentiment>'\n",
    "task = torch.tensor([task] * batch_size)\n",
    "task.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, lengths = to_input_tensor(lang, sents, device)\n",
    "x = x.transpose(0, 1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 20\n",
    "w_embedding = nn.Embedding(lang.n_words, embed_dim)\n",
    "t_embedding = nn.Embedding(1, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = t_embedding(task).unsqueeze(-1)\n",
    "te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 30\n",
    "linear = nn.Linear(embed_dim, hidden_dim)\n",
    "classify = nn.Linear(hidden_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xe = w_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.3\n",
    "mha = nn.MultiheadAttention(embed_dim, 1, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 30])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xa, _ = mha(xe, xe, xe)\n",
    "x = linear(xa)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.bmm(x, te)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.3235e-06],\n",
       "        [2.1673e-07],\n",
       "        [1.1966e-03],\n",
       "        [2.6087e-04],\n",
       "        [3.0807e-03],\n",
       "        [1.4841e-07],\n",
       "        [7.4762e-01],\n",
       "        [2.8914e-06],\n",
       "        [5.5351e-03],\n",
       "        [5.8624e-07],\n",
       "        [2.2333e-04],\n",
       "        [1.6494e-01],\n",
       "        [2.8781e-06],\n",
       "        [7.7139e-02]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(w.squeeze(-1), -1).unsqueeze(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 30])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_attention = w * x\n",
    "weighted_attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 14])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wa = weighted_attention.transpose(1, 2)\n",
    "wa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.2636e-04],\n",
       "        [4.7082e-02],\n",
       "        [5.3003e-05]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxpool, _ = torch.max(wa, -1)\n",
    "y = torch.sigmoid(classify(maxpool))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskSpecificAttention(nn.Module):\n",
    "    def __init__(self, language, device, embed_dim, hidden_dim, num_embed, num_heads, num_layers, dropout, n_classes):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.language = language\n",
    "        \n",
    "        self.w_embedding = nn.Embedding(lang.n_words, embed_dim)\n",
    "        self.t_embedding = nn.Embedding(num_layers, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.mhas, self.linear_1, self.linear_2 = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        self.ln_1, self.ln_2 = nn.ModuleList(), nn.ModuleList()\n",
    "        self.tasks = []\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.mhas.append(nn.MultiheadAttention(embed_dim, 1, dropout=dropout))\n",
    "            self.linear_1.append(nn.Linear(embed_dim, hidden_dim))\n",
    "            self.linear_2.append(nn.Linear(hidden_dim, embed_dim))\n",
    "            self.tasks.append(i)\n",
    "            \n",
    "            self.ln_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.ln_2.append(nn.LayerNorm(hidden_dim, eps=1e-12))\n",
    "        \n",
    "        self.classify = nn.Linear(embed_dim, n_classes)\n",
    "        \n",
    "    def forward(self, sents):\n",
    "        batch_size = len(sents)\n",
    "        x, lengths = to_input_tensor(lang, sents, device)\n",
    "        x = x.transpose(0, 1)\n",
    "        # bs, seq, embed\n",
    "        x = self.w_embedding(x)\n",
    "\n",
    "        for task, mha, linear_1, linear_2, lnorm_1, lnorm_2 in zip(self.tasks, self.mhas, self.linear_1, self.linear_2, self.ln_1, self.ln_2):\n",
    "            tasks = torch.tensor([task] * batch_size)\n",
    "            te = self.t_embedding(tasks).unsqueeze(-1)\n",
    "            \n",
    "            x = lnorm_1(x)\n",
    "            # bs, seq, embed\n",
    "            x, _ = mha(x, x, x)\n",
    "            # bs, seq, hidden\n",
    "            x = linear_1(x)\n",
    "            \n",
    "            # task attention\n",
    "            w = torch.bmm(x, te)\n",
    "            w = torch.softmax(w.squeeze(-1), -1).unsqueeze(-1)\n",
    "            weighted_attention = (w * x)\n",
    "            x = self.dropout(weighted_attention)\n",
    "            \n",
    "            x = lnorm_2(x)\n",
    "            # bs, seq, embed\n",
    "            x = F.relu(linear_2(x))\n",
    "\n",
    "        # bs, embed, seq\n",
    "        x = x.transpose(1, 2)\n",
    "        maxpool, _ = torch.max(x, -1)\n",
    "        y = torch.sigmoid(self.classify(maxpool)).squeeze()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 1\n",
    "n_layers = 2\n",
    "dropout = 0.3 \n",
    "\n",
    "model = TaskSpecificAttention(lang, device, embed_dim, hidden_dim, lang.n_words, n_heads, n_layers, dropout, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4007, 0.4548, 0.4895], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
